1. Use retail_db from https://github.com/erkansirin78/datasets/tree/master/retail_db

2. Import all retail_db into mysql azhadoop database

3. Using sqoop import all tables into hive table 

4. Using apache spark find most cancelled 10 categories in total price

5. write results into hive table 



Create mysql tables 
create table products(productId int, productCategoryId int, productName VARCHAR(255), productDescription VARCHAR(255), productPrice double, productImage VARCHAR(255));


create table orders(orderId int, orderDate VARCHAR(50), orderCustomerId int, orderStatus VARCHAR(20));


create table order_items(orderItemName int, orderItemOrderId int, orderItemProductId int, orderItemQuantity int, orderItemSubTotal double, orderItemProductPrice double);


create table departments(departmentId int, departmentName VARCHAR(20));

create table customers(customerId int, customerFName VARCHAR(50), customerLName VARCHAR(50), customerEmail VARCHAR(50), customerPassword VARCHAR(50), customerStreet VARCHAR(50), customerCity VARCHAR(50), customerState VARCHAR(5), customerZipcode VARCHAR(15));


create table categories(categoryId int, categoryDepartmentId int, categoryName VARCHAR(50));


INSERT DATA MYSQL

LOAD DATA LOCAL INFILE '/root/retail_db/products.csv' INTO TABLE products
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;


LOAD DATA LOCAL INFILE '/root/retail_db/orders.csv' INTO TABLE orders
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;


LOAD DATA LOCAL INFILE '/root/retail_db/order_items.csv' INTO TABLE order_items
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;

LOAD DATA LOCAL INFILE '/root/retail_db/departments.csv' INTO TABLE departments
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;



LOAD DATA LOCAL INFILE '/root/retail_db/customers.csv' INTO TABLE customers
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;



LOAD DATA LOCAL INFILE '/root/retail_db/categories.csv' INTO TABLE categories
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;




sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from categories WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.categories --target-dir /tmp/categories



sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from customers WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.customers --target-dir /tmp/customers



sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from departments WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.departments --target-dir /tmp/departments



sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from order_items WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.order_items --target-dir /tmp/order_items




sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from orders WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.orders --target-dir /tmp/orders


sqoop import --connect jdbc:mysql://sandbox-hdp.hortonworks.com/azhadoop \
--driver com.mysql.jdbc.Driver \
--username root --password hadoop \
--query "select * from products WHERE \$CONDITIONS" \
--m 1 --hive-import --create-hive-table \
--hive-table azhadoop.products --target-dir /tmp/products
