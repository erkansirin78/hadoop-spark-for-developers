1. Veri bilimciler için Jupyter, Zeppelin gibi Notebooklar'da kod geliştirmek daha kolaydır. Notebook'lar IDE'nin karmaşıklığından uzak daha 
sade bir kod geliştirme deneyimi sunar. 

2. Notebook kullanılırken çok fazla parağraf kullanılmamalıdır. 
Belli bir sayıya ulaşınca (örneğin 30) denenip sonucu gözlenen kodlar aynı parağrafta birleştirilebilir.

3. Kod geliştirme tamamlanınca kodlar gerçek ortamda çalıştırılmak üzere ilgili dilin uzantılı dosyasında birleştirilir. Örneğin .py uzantısıyla.
.scala kodları maven veya sbt ile derlenir ve .jar haline getirilir.

4. 
spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
  
Basit bir pyspark uygulaması submit örneği:

spark-submit --master yarn kod_dosyasi.py 

5. Spark'ın kaynakları dinamik olarak alıp bırakması:
https://community.hortonworks.com/content/supportkb/49510/how-to-enable-dynamic-resource-allocation-in-spark.html
yazıdaki parametre değişiklikleri ile yapılabilir.


Custom Spar2 defaults

spark.dynamicAllocation.enabled = true 
spark.shuffle.service.enabled = true 
spark.dynamicAllocation.initialExecutors = 3 (Initial number of executors to run if dynamic allocation is enabled, this is same as "spark.dynamicAllocation.minExecutors") 
spark.dynamicAllocation.minExecutors = 3 (executors number will come to this number if executors are not in use, after 60 sec(default), controlled by "spark.dynamicAllocation. executorIdleTimeout") 
spark.dynamicAllocation.maxExecutors = 30 (maximum executors that job can request)



eklenir.

Bu ayarlar eklendikten sonra spark-submit te ayrıca kaynak belirlemeye gerek yoktur.

